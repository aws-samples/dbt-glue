{% macro glue__py_write_table(compiled_code, target_relation) %}
{{ compiled_code }}

# --- Autogenerated dbt materialization code. --- #
from pyspark.sql import SparkSession
from pyspark.sql.functions import *

# Get or create SparkSession
spark = SparkSession.builder.getOrCreate()

# Set up the schema and model name
schema = "{{ target_relation.schema }}"
model_name = "{{ target_relation.identifier }}"
print("DEBUG: Setting up model in schema: " + schema)

class dbtObj:
    def __init__(self, table_function, schema, model_name):
        self.table_function = table_function
        self.schema = schema
        self.model_name = model_name
        self.this = schema + "." + model_name
        
    def config(self, **config_args):
        # This is a placeholder for dbt config in Python models
        print("DEBUG: dbt.config called with:", config_args)
        pass
        
    def ref(self, name):
        return self.table_function(name)
        
    def source(self, source_name, table_name):
        return self.table_function(source_name + "." + table_name)
        
    def is_incremental(self):
        # Check if the target table exists by trying to query it
        try:
            spark.sql("SELECT 1 FROM " + schema + "." + model_name + " LIMIT 1")
            print("DEBUG: Table exists, this is an incremental run")
            return True
        except Exception as e:
            print("DEBUG: Table does not exist, this is a full refresh run:", str(e))
            return False

# Initialize dbt object
dbt = dbtObj(spark.table, schema, model_name)

# Execute the model function
print("DEBUG: About to execute model function")
df = None

try:
    print("DEBUG: Calling model function...")
    df = model(dbt, spark)
    print("DEBUG: Model function executed successfully, got DataFrame: " + str(type(df)))
except NameError as e:
    print("DEBUG: NameError when calling model:", e)
    try:
        print("DEBUG: Trying main function as fallback...")
        df = main(dbt, spark)
        print("DEBUG: Main function executed successfully, got DataFrame: " + str(type(df)))
    except NameError as e2:
        print("DEBUG: NameError when calling main:", e2)
        print("DEBUG: Available functions:")
        import types
        for name, obj in globals().items():
            if isinstance(obj, types.FunctionType) and not name.startswith('_'):
                print("  -", name, ":", type(obj))
        raise Exception("Neither 'model' nor 'main' function found in the Python code")
except Exception as e:
    print("DEBUG: Exception when calling model function:", e)
    import traceback
    traceback.print_exc()
    raise e

# Validate that we got a DataFrame
import pyspark.sql.dataframe
if not isinstance(df, pyspark.sql.dataframe.DataFrame):
    raise Exception("Model function must return a Spark DataFrame, got " + str(type(df)))

# Write the DataFrame to the target table
print("DEBUG: Writing DataFrame to table " + schema + "." + model_name)
writer = df.write.mode("overwrite").option("overwriteSchema", "true")

# Apply file format and other options from config
{%- set file_format = config.get('file_format', 'parquet') -%}
{%- set partition_by = config.get('partition_by', none) -%}
{%- set custom_location = config.get('custom_location', '') -%}

{%- if file_format %}
writer = writer.format("{{ file_format }}")
{%- endif %}

{%- if partition_by is not none -%}
    {%- if partition_by is string -%}
        {%- set partition_by = [partition_by] -%}
    {%- endif %}
writer = writer.partitionBy({{ partition_by | tojson }})
{%- endif %}

{%- if custom_location %}
writer = writer.option("path", "{{ custom_location }}")
{%- endif %}

# Save the table
writer.saveAsTable("{{ target_relation }}")

# Refresh the table to make it available
spark.sql("REFRESH TABLE {{ target_relation }}")

print("DEBUG: Successfully wrote table {{ target_relation }}")
print("DEBUG: Python model execution completed successfully")
{% endmacro %}

{% macro glue__py_get_writer_options() %}
    {%- set file_format = config.get('file_format', 'parquet') -%}
    {%- set location = adapter.get_location(this) -%}
    {%- set partition_by = config.get('partition_by', none) -%}
    {%- set table_properties = config.get('table_properties', {}) -%}

    {%- set options = {
        'file_format': file_format,
        'location': location,
        'partition_by': partition_by,
        'table_properties': table_properties
    } -%}

    {{ return(options) }}
{% endmacro %}

{% macro glue__create_python_merge_table(model, df_name, unique_key) %}
    {%- set target_relation = this.incorporate(type='table') -%}
    {%- set temp_view = model['name'] + '_temp_view' -%}

    -- Create a temporary view of the new data
    {{ df_name }}.createOrReplaceTempView('{{ temp_view }}')

    -- Merge the data using the unique key
    MERGE INTO {{ target_relation }} AS target
    USING {{ temp_view }} AS source
    ON {% for key in unique_key %}
        target.{{ key }} = source.{{ key }}
        {% if not loop.last %} AND {% endif %}
    {% endfor %}
    WHEN MATCHED THEN
        UPDATE SET
        {% for column in adapter.get_columns_in_relation(target_relation) %}
            {% if column.name not in unique_key %}
                target.{{ column.name }} = source.{{ column.name }}
                {% if not loop.last %},{% endif %}
            {% endif %}
        {% endfor %}
    WHEN NOT MATCHED THEN
        INSERT *
{% endmacro %}

{% macro glue__create_python_intermediate_table(model, df_name) %}
    {%- set relation = this.incorporate(type='table') -%}
    {%- set dest_columns = adapter.get_columns_in_relation(this) -%}

    {%- set write_mode = 'overwrite' if should_full_refresh() else 'append' -%}
    {%- set location = adapter.get_location(this) -%}

    {{ df_name }}.createOrReplaceTempView('{{ relation.identifier }}_intermediate')

    CREATE TABLE {{ relation }} AS
    SELECT * FROM {{ relation.identifier }}_intermediate
{% endmacro %}
